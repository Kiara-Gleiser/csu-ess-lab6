---
title: "Lab 6: Machine Learning in Hydrology"
author: "Kiara Gleiser"
date: "2025-04-04"
format: html
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(ggthemes)
library(patchwork)
library(nnet)
library(workflowsets)
```

## Q1: Download data 

```{r}
root <- 'https://gdex.ucar.edu/dataset/camels/file'
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('data/camels_{types}.txt')

dir.create("data", showWarnings = FALSE)

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

walk2(remote_files, local_files, download.file, quiet = TRUE)

camels <- map(local_files, read_delim, show_col_types = FALSE) |> 
  power_full_join(by = 'gauge_id')
```

`zero_q_freq` represents the **frequency of zero-flow days** — a measure of how often the stream at the gauge has no flow.

## Q2: Make 2 maps 

```{r}
map1 <- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "khaki1", high = "darkolivegreen") +
  labs(title = "Aridity Index by Basin") +
  theme_map()

map2 <- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Mean Precipitation by Basin") +
  theme_map()

map1 + map2
```

## Q3: Add XGBoost & Neural Net

```{r}
set.seed(123)
camels <- camels |> mutate(logQmean = log(q_mean))
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)
camels_cv <- vfold_cv(camels_train, v = 10)

rec <- recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ aridity:p_mean) %>%
  step_naomit(all_predictors(), all_outcomes())

xgb_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

nnet_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")
```

## Q4a: Data Splitting 

*Done above with initial_split and vfold_cv.*

## Q4b: Recipe 

*I chose `aridity` and `p_mean` because they are primary hydroclimate indicators. Both are skewed and interact non-linearly, so log transforms and interactions help model the system.*

*Defined above as `rec`.*

## Q4c: Define 3 Models

```{r}
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```

*3 models: `lm_model`, `xgb_model`, and `nnet_model`.*

## Q4d: Workflow Set 

```{r}
wf <- workflow_set(
  preproc = list(rec),
  models = list(
    linear_reg = lm_model,
    xgboost = xgb_model,
    neural_net = nnet_model
  )
) %>%
  workflow_map("fit_resamples", resamples = camels_cv, verbose = TRUE)

wf_status <- wf %>%
  mutate(success = map_lgl(result, ~ inherits(.x, "resample_results")))

print(wf_status %>% select(wflow_id, success))

wf_clean <- wf_status %>% filter(success)
```

## Q4e/Q4f: Evaluation and Extract
```{r}
if (exists("wf_clean") && is.data.frame(wf_clean) && nrow(wf_clean) > 0) {
  autoplot(wf_clean)
  
  best_model_info <- rank_results(wf_clean, rank_metric = "rsq", select_best = TRUE)
  print(best_model_info)
  
  best_id <- best_model_info$wflow_id[[1]]
} else {
  stop("No workflows returned results — check your models or preprocessing.")
}

## Check model inputs and download CAMELS if needed
if (!exists("camels") | !exists("camels_train") | !exists("rec")) {
  library(glue)
  root <- 'https://gdex.ucar.edu/dataset/camels/file'
  types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
  remote_files <- glue('{root}/camels_{types}.txt')
  local_files  <- glue('data/camels_{types}.txt')
  dir.create("data", showWarnings = FALSE)
  
  walk2(remote_files, local_files, download.file, quiet = TRUE)
  
  camels <- map(local_files, read_delim, show_col_types = FALSE) |> 
    reduce(powerjoin::power_full_join, by = "gauge_id") |> 
    mutate(logQmean = log(q_mean))
  
  set.seed(123)
  camels_split <- initial_split(camels, prop = 0.8)
  camels_train <- training(camels_split)
  camels_test  <- testing(camels_split)
  camels_cv    <- vfold_cv(camels_train, v = 10)
  
  rec <- recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
    step_log(all_predictors()) %>%
    step_interact(terms = ~ aridity:p_mean) %>%
    step_naomit(all_predictors(), all_outcomes())
}

## Fit best model and evaluate
model_lookup <- list(
  linear_reg = linear_reg() |> set_engine("lm") |> set_mode("regression"),
  xgboost    = boost_tree() |> set_engine("xgboost") |> set_mode("regression"),
  neural_net = bag_mlp()    |> set_engine("nnet") |> set_mode("regression")
)

# Fix: Extract just the model name from best_id
model_name <- str_remove(best_id, "^recipe_")

# Now use it to look up the model
if (model_name %in% names(model_lookup)) {
  best_model <- workflow() %>%
    add_recipe(rec) %>%
    add_model(model_lookup[[model_name]]) %>%
    fit(data = camels_train)

  preds <- augment(best_model, new_data = camels_test)

  print(metrics(preds, truth = logQmean, estimate = .pred))

  preds |> 
    ggplot(aes(x = logQmean, y = .pred, color = aridity)) +
    geom_point() +
    geom_abline(linetype = 2) +
    scale_color_viridis_c() +
    labs(
      title = paste0(model_name, ": Observed vs Predicted Streamflow"),
      x = "Observed Log Mean Flow",
      y = "Predicted Log Mean Flow",
      color = "Aridity"
    ) +
    theme_linedraw()
} else {
  stop(paste0("Model name '", model_name, "' not found in model lookup."))
}
```
